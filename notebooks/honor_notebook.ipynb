{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. looking at 'simplified data' input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"raw_data/full_simplified_The Eiffel Tower.ndjson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/honor/code/rs-uk/pictionary-ai\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'word': 'The Eiffel Tower', 'countrycode': 'GB', 'timestamp': '2017-03-11 14:47:44.05242 UTC', 'recognized': True, 'key_id': '5027286841556992', 'drawing': [[[0, 22, 37, 64, 255], [218, 220, 227, 228, 211]], [[76, 95, 135, 141, 150, 159, 166, 180, 186, 201], [220, 138, 31, 0, 63, 79, 117, 150, 191, 224]], [[94, 104, 111, 119, 127, 141, 143, 142, 180, 191], [212, 167, 149, 80, 59, 41, 30, 134, 202, 232]], [[109, 127, 137, 147, 150, 162, 172, 185], [122, 120, 104, 97, 99, 124, 128, 128]], [[75, 130, 158], [162, 159, 150]]]}\n",
      "<class 'dict'>\n",
      "{'word': 'The Eiffel Tower', 'countrycode': 'FR', 'timestamp': '2017-03-12 22:51:18.52595 UTC', 'recognized': True, 'key_id': '5716269791707136', 'drawing': [[[47, 47, 36, 26, 0, 10, 23, 46, 46, 63, 68, 105, 107, 112, 134, 152, 170, 151, 125, 101, 82, 74, 70, 60], [18, 98, 167, 201, 255, 248, 245, 248, 235, 183, 181, 187, 202, 216, 244, 252, 239, 180, 140, 93, 42, 1, 1, 21]], [[76, 68, 78, 101, 102, 93], [71, 131, 135, 136, 126, 101]]]}\n",
      "<class 'dict'>\n",
      "{'word': 'The Eiffel Tower', 'countrycode': 'GB', 'timestamp': '2017-03-29 01:12:00.36798 UTC', 'recognized': True, 'key_id': '5942899998982144', 'drawing': [[[184, 115, 67, 57, 36, 18], [251, 103, 12, 109, 193, 247]], [[145, 154, 150, 48, 24], [180, 176, 175, 178, 173]], [[55, 50, 38, 0, 28, 31, 60, 66, 71], [128, 152, 179, 253, 254, 247, 114, 26, 0]], [[72, 76, 81, 83, 86, 99, 93, 86], [26, 35, 71, 250, 255, 254, 189, 5]], [[67, 74, 97, 172], [12, 16, 49, 194]]]}\n",
      "<class 'dict'>\n",
      "{'word': 'The Eiffel Tower', 'countrycode': 'US', 'timestamp': '2017-03-29 16:48:54.9129 UTC', 'recognized': True, 'key_id': '6226163091374080', 'drawing': [[[0, 187, 177, 132, 105, 79, 38, 19, 11], [248, 245, 241, 153, 114, 153, 202, 245, 255]], [[100, 99, 107, 107, 101, 96], [113, 109, 105, 89, 9, 0]], [[73, 145], [161, 162]], [[56, 52, 61, 139, 163], [204, 203, 200, 199, 193]], [[46, 94, 188], [217, 213, 212]], [[19, 18, 25, 39], [245, 242, 239, 236]]]}\n",
      "<class 'dict'>\n",
      "{'word': 'The Eiffel Tower', 'countrycode': 'GB', 'timestamp': '2017-03-04 15:50:45.19801 UTC', 'recognized': True, 'key_id': '4889008825958400', 'drawing': [[[0, 21, 43, 83, 97, 158, 169, 172], [162, 163, 157, 134, 121, 36, 14, 0]], [[228, 189, 182, 173, 170], [182, 127, 110, 69, 20]], [[120, 125, 148, 175, 191], [56, 52, 48, 47, 50]], [[109, 196], [92, 79]], [[96, 168, 192, 206], [124, 113, 113, 118]], [[61, 92, 135, 212, 255], [145, 149, 150, 139, 137]], [[174, 156, 152], [24, 101, 171]]]}\n",
      "<class 'dict'>\n",
      "{'word': 'The Eiffel Tower', 'countrycode': 'US', 'timestamp': '2017-01-25 13:20:22.01707 UTC', 'recognized': True, 'key_id': '5643506171248640', 'drawing': [[[131, 119, 48, 26], [16, 56, 221, 255]], [[137, 179, 196, 207], [0, 185, 236, 255]], [[48, 67, 146, 190], [208, 210, 200, 198]], [[18, 1, 0, 61, 83], [254, 226, 215, 142, 110]], [[84, 79, 77, 102, 119], [113, 114, 111, 64, 11]]]}\n",
      "<class 'dict'>\n",
      "{'word': 'The Eiffel Tower', 'countrycode': 'IT', 'timestamp': '2017-03-26 16:45:12.19889 UTC', 'recognized': True, 'key_id': '5048121811795968', 'drawing': [[[0, 37, 58, 74, 92, 105, 106], [238, 227, 211, 190, 143, 60, 28]], [[106, 109, 130, 130, 127, 104, 117, 128], [27, 36, 34, 29, 26, 27, 0, 26]], [[132, 131, 168, 187, 255, 212, 162, 121, 113, 88, 14], [27, 57, 176, 196, 230, 233, 196, 173, 186, 206, 234]]]}\n"
     ]
    }
   ],
   "source": [
    "#loading jason file\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        json_line = json.loads(line)\n",
    "        print(type(json_line))\n",
    "        print(json_line)\n",
    "        if i > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import utils\n",
    "from keras import Model, Sequential, layers, regularizers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(tensor, max_length, padding='pre'):\n",
    "    '''function is going to take in the tensor and padd the data to max_length, return a tensor that is padded, defult is pre padding'''\n",
    "    padded_tensor =utils.pad_sequence(tensor, maxlen=max_length, padding=padding)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data from big query into dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is not correct, i am receiving in a different format and want to output a different format\n",
    "\n",
    "def get_data_with_cache(\n",
    "        gcp_project:str,\n",
    "        query:str,\n",
    "        cache_path:Path,\n",
    "        data_has_header=True\n",
    "    ) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve `query` data from BigQuery, or from `cache_path` if the file exists\n",
    "    Store at `cache_path` if retrieved from BigQuery for future use\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tensor = padded_tensor()\n",
    "\n",
    "tensor_length = len(padded_tensor)\n",
    "train_length = int(0.7 * tensor_length)\n",
    "test_length = tensor_length- train_length\n",
    "\n",
    "X_train = padded_tensor[:train_length,]\n",
    "X_test = padded_tensor[train_length:,]\n",
    "\n",
    "y_test = 0.3 *\n",
    "y_train =\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bidirectional(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights, using bidirectional LTSM\n",
    "    masking layer\n",
    "    it has 2 Bidirectional LSTM layers\n",
    "    3 dense layers\n",
    "    and dropout layers\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Masking layer to handle variable-length sequences\n",
    "    #put in 99 as 0 may effect the data\n",
    "    model.add(layers.Masking(mask_value=99, input_shape=(max_length, 3)))\n",
    "\n",
    "    #do we want to customize backwards layer?\n",
    "\n",
    "    model.add(layers.Bidirectional(layers.LSTM(196, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "    # Add Dense layers\n",
    "    model.add(layers.Dense(128, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(64, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(32, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    # Add final Softmax layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    # Replace 'num_classes' with the actual number of classes in your problem\n",
    "\n",
    "    print(\"✅ Model initialized\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_LTSM(input_shape: tuple) -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    model that just has LSTM same structure otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Masking layer to handle variable-length sequences\n",
    "    #put in 99 as 0 may effect the data\n",
    "    model.add(layers.Masking(mask_value=99, input_shape=(max_length, 3)))\n",
    "\n",
    "    # Add LSTM layers\n",
    "    model.add(layers.LSTM(64, activation='tanh', return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(layers.LSTM(32, activation='tanh', return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "\n",
    "    # Add Dense layers\n",
    "    model.add(layers.Dense(128, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(64, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(32, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    # Add final Softmax layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    # Replace 'num_classes' with the actual number of classes in your problem\n",
    "\n",
    "    print(\"✅ Model initialized\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_LTSM_conv(self):\n",
    "      '''model has conv1d layer and max pooling adn than LTSM, got this model from\n",
    "      https://medium.com/@www.seymour/training-a-recurrent-neural-network-to-recognise-sketches-in-a-real-time-game-of-pictionary-16c91e185ce6'''\n",
    "      model = Sequential()\n",
    "\n",
    "      # Input layer\n",
    "      model.add(layers.Input(shape=self.state_shape))\n",
    "\n",
    "      # Masking layer\n",
    "      model.add(layers.Masking(mask_value=99))\n",
    "\n",
    "      # 1D Convolutional Layers- should i have more or less dropout?\n",
    "      model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "      model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "      model.add(layers.MaxPooling1D(2))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "      model.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "      model.add(layers.MaxPooling1D(2))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "      # Recurrent layers (e.g., LSTM)\n",
    "      model.add(layers.LSTM(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "      model.add(layers.LSTM(128,return_sequences=True,dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "\n",
    "      # Dense layers\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "      # Output layer\n",
    "      model.add(layers.Dense(self.num_categories, activation='softmax'))\n",
    "\n",
    "      return model\n",
    "\n",
    "def compile_model(model: Model, learning_rate=0.0005) -> Model:\n",
    "    \"\"\"\n",
    "    Compile the Neural Network\n",
    "    with loss categorical_crossentropy, optimiser adam, metrics, accuracy\n",
    "    \"\"\"\n",
    "    #what loss do we want?\n",
    "    #i think should be using categorical\n",
    "    #which metrics?\n",
    "    #do i want to create my own and what are the advantages of this\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    # look at custum loss function\n",
    "    print(\"✅ Model compiled\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "        model: Model,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        batch_size=256,\n",
    "        patience=30,\n",
    "        validation_data=None, # overrides validation_split\n",
    "        validation_split=0.3\n",
    "    ) -> Tuple[Model, dict]:\n",
    "    \"\"\"\n",
    "    Fit the model and return a tuple (fitted_model, history)\n",
    "    added in checkpoint as well\n",
    "    \"\"\"\n",
    "    print(Fore.BLUE + \"\\nTraining model...\" + Style.RESET_ALL)\n",
    "\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = '/home/honor/code/rs-uk/pictionary-ai/raw_data/models'\n",
    "    #this will save the checkpoints in the checkpoint_filepath\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "    #in fit is where we put in the padding, cant remember how\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_data=validation_data,\n",
    "        validation_split=validation_split,\n",
    "        epochs=100,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es, model_checkpoint_callback],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Model trained on {len(X)} rows with min val accuracy: {round(np.min(history.history['accuracy']), 2)}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "        model: Model,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        batch_size=64\n",
    "    ) -> Tuple[Model, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate trained model performance on the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.BLUE + f\"\\nEvaluating model on {len(X)} rows...\" + Style.RESET_ALL)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"\\n❌ No model to evaluate\")\n",
    "        return None\n",
    "\n",
    "    metrics = model.evaluate(\n",
    "        x=X,\n",
    "        y=y,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        # callbacks=None,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    loss = metrics[\"loss\"]\n",
    "    mae = metrics[\"accuracy\"]\n",
    "\n",
    "    print(f\"✅ Model evaluated, accuracy: {round(mae, 2)}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model\n",
    "model = model_bidirectional()\n",
    "#compile model\n",
    "model= compile_model(model)\n",
    "#train model\n",
    "model, history = train_model(model, X_train, y_train, validation_data=[X_val,y_val])\n",
    "#evaluate model\n",
    "metrics=evaluate_model(model, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pictionary-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
