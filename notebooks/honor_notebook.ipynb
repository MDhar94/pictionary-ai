{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. looking at 'simplified data' input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"raw_data/full_simplified_The Eiffel Tower.ndjson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/honor/code/rs-uk/pictionary-ai/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'raw_data/full_simplified_The Eiffel Tower.ndjson'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#loading jason file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m      4\u001b[0m         json_line \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'raw_data/full_simplified_The Eiffel Tower.ndjson'"
     ]
    }
   ],
   "source": [
    "#loading jason file\n",
    "with open(file_path, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        json_line = json.loads(line)\n",
    "        print(type(json_line))\n",
    "        print(json_line)\n",
    "        if i > 5:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (2.15.0.post1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (63.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (1.62.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/honor/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 15:11:24.197983: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-08 15:11:24.419088: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-08 15:11:24.425092: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-03-08 15:11:24.425114: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-03-08 15:11:24.488005: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-08 15:11:25.666026: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-08 15:11:25.666115: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-03-08 15:11:25.666120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import callbacks\n",
    "from tensorflow.keras import utils\n",
    "from keras import Model, Sequential, layers, regularizers, optimizers\n",
    "from sklearn.preprocessing import TargetEncoder\n",
    "from colorama import Fore, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(tensor, max_length, padding='post'):\n",
    "    '''function is going to take in the tensor and padd the data to max_length, return a tensor that is padded, defult is post padding'''\n",
    "    padded_tensor =utils.pad_sequence(tensor, maxlen=max_length, padding=padding)\n",
    "    return padded_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m Y \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/honor/code/rs-uk/pictionary-ai/raw_data/data_y.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/honor/code/rs-uk/pictionary-ai/raw_data/data_X.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1895\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:93\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype_backend\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[1;32m     92\u001b[0m     import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[43mparsers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32mparsers.pyx:574\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mparsers.pyx:721\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._get_header\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Y = pd.read_csv('/home/honor/code/rs-uk/pictionary-ai/raw_data/data_y.csv')\n",
    "X = pd.read_csv(\"/home/honor/code/rs-uk/pictionary-ai/raw_data/data_X.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function worked for X dunno what it does\n",
    "XX = np.genfromtxt(\"../raw_data/data_X.csv\", delimiter=\",\")\n",
    "XX = XX.reshape((14352,150,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14352,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this function worked for y as the one above wasnt working, has to have delimiter and dtype\n",
    "yy =np.loadtxt(\"../raw_data/data_y.csv\", dtype=str, delimiter=\",\")\n",
    "yy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"'The Great Wall of China'\", \"'The Great Wall of China'\",\n",
       "       \"'The Great Wall of China'\", \"'The Great Wall of China'\",\n",
       "       \"'The Great Wall of China'\"], dtype='<U25')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#works but gives wierd format\n",
    "Y = pd.read_csv('/home/honor/code/rs-uk/pictionary-ai/raw_data/data_y.csv').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length mismatch: Expected axis has 0 elements, new values have 1 elements",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mY\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m \u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/core/generic.py:6310\u001b[0m, in \u001b[0;36mNDFrame.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   6308\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   6309\u001b[0m     \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name)\n\u001b[0;32m-> 6310\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__setattr__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6311\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m   6312\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32mproperties.pyx:69\u001b[0m, in \u001b[0;36mpandas._libs.properties.AxisProperty.__set__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/core/generic.py:813\u001b[0m, in \u001b[0;36mNDFrame._set_axis\u001b[0;34m(self, axis, labels)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    809\u001b[0m \u001b[38;5;124;03mThis is called from the cython code when we set the `index` attribute\u001b[39;00m\n\u001b[1;32m    810\u001b[0m \u001b[38;5;124;03mdirectly, e.g. `series.index = [1, 2, 3]`.\u001b[39;00m\n\u001b[1;32m    811\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    812\u001b[0m labels \u001b[38;5;241m=\u001b[39m ensure_index(labels)\n\u001b[0;32m--> 813\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/core/internals/managers.py:238\u001b[0m, in \u001b[0;36mBaseBlockManager.set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_axis\u001b[39m(\u001b[38;5;28mself\u001b[39m, axis: AxisInt, new_labels: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# Caller is responsible for ensuring we have an Index object.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_set_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis] \u001b[38;5;241m=\u001b[39m new_labels\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/pictionary-ai/lib/python3.10/site-packages/pandas/core/internals/base.py:98\u001b[0m, in \u001b[0;36mDataManager._validate_set_axis\u001b[0;34m(self, axis, new_labels)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m new_len \u001b[38;5;241m!=\u001b[39m old_len:\n\u001b[0;32m---> 98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength mismatch: Expected axis has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements, new \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues have \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_len\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m elements\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length mismatch: Expected axis has 0 elements, new values have 1 elements"
     ]
    }
   ],
   "source": [
    "Y.columns = ['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan, ..., nan, nan, nan])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this was when using genfromtxt\n",
    "yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6458400,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# target encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoder = OneHotEncoder(sparse_output=False)\n",
    "#terget encoding, than transforming y which is the classes\n",
    "y_encoded =target_encoder.fit_transform(yy.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14352, 2)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tensor = XX\n",
    "\n",
    "tensor_length = len(padded_tensor)\n",
    "train_length = int(0.7 * tensor_length)\n",
    "test_length = tensor_length- train_length\n",
    "\n",
    "#taking in the padded X data and splititng it 70 30\n",
    "X_train = padded_tensor[:train_length,]\n",
    "X_test = padded_tensor[train_length:,]\n",
    "\n",
    "#taking in y encoded and spliting it 70 30\n",
    "y_train = y_encoded[:train_length]\n",
    "y_test = y_encoded[train_length:]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the length we are padding too\n",
    "max_length = 150\n",
    "\n",
    "#no of classes we are using\n",
    "num_classes = 10\n",
    "def model_bidirectional() -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights, using bidirectional LTSM\n",
    "    masking layer\n",
    "    it has 2 Bidirectional LSTM layers\n",
    "    3 dense layers\n",
    "    and dropout layers\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Masking layer to handle variable-length sequences\n",
    "    #put in 99 as 0 may effect the data\n",
    "    model.add(layers.Masking(mask_value=99, input_shape=(max_length, 3)))\n",
    "\n",
    "    #do we want to customize backwards layer?\n",
    "\n",
    "    model.add(layers.Bidirectional(layers.LSTM(196, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "    # Add Dense layers\n",
    "    model.add(layers.Dense(128, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(64, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(32, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    # Add final Softmax layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    # Replace 'num_classes' with the actual number of classes in your problem\n",
    "\n",
    "    print(\"✅ Model initialized\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_LTSM() -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights\n",
    "    model that just has LSTM same structure otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Masking layer to handle variable-length sequences\n",
    "    #put in 99 as 0 may effect the data\n",
    "    model.add(layers.Masking(mask_value=99, input_shape=(max_length, 3)))\n",
    "\n",
    "    # Add LSTM layers\n",
    "    model.add(layers.LSTM(64, activation='tanh', return_sequences=True, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(layers.LSTM(32, activation='tanh', dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "\n",
    "    # Add Dense layers\n",
    "    model.add(layers.Dense(128, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(64, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(32, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    # Add final Softmax layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    # Replace 'num_classes' with the actual number of classes in your problem\n",
    "\n",
    "    print(\"✅ Model initialized\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def model_LTSM_conv():\n",
    "      '''model has conv1d layer and max pooling adn than LTSM, got this model from\n",
    "      https://medium.com/@www.seymour/training-a-recurrent-neural-network-to-recognise-sketches-in-a-real-time-game-of-pictionary-16c91e185ce6'''\n",
    "      model = Sequential()\n",
    "\n",
    "      # Input layer\n",
    "      model.add(layers.Masking(mask_value=99, input_shape=(max_length, 3)))\n",
    "\n",
    "\n",
    "      # 1D Convolutional Layers- should i have more or less dropout?\n",
    "      model.add(layers.Conv1D(32, 3, activation='relu'))\n",
    "      model.add(layers.Conv1D(64, 3, activation='relu'))\n",
    "      model.add(layers.MaxPooling1D(2))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "      model.add(layers.Conv1D(128, 3, activation='relu'))\n",
    "      model.add(layers.MaxPooling1D(2))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "      # Recurrent layers (e.g., LSTM)\n",
    "      model.add(layers.LSTM(128, return_sequences=True,dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "      model.add(layers.LSTM(128,dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "\n",
    "      # Dense layers\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "      model.add(layers.Dense(128, activation='relu'))\n",
    "      model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "      # Output layer\n",
    "      model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "      return model\n",
    "\n",
    "def compile_model(model: Model, learning_rate=0.0005) -> Model:\n",
    "    \"\"\"\n",
    "    Compile the Neural Network\n",
    "    with loss categorical_crossentropy, optimiser adam, metrics, accuracy\n",
    "    \"\"\"\n",
    "    #what loss do we want?\n",
    "    #i think should be using categorical\n",
    "    #which metrics?\n",
    "    #do i want to create my own and what are the advantages of this\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "    # look at custum loss function\n",
    "    print(\"✅ Model compiled\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_model(\n",
    "        model: Model,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        batch_size=256,\n",
    "        patience=3,\n",
    "        validation_data=None, # overrides validation_split\n",
    "        validation_split=0.3\n",
    "    ) -> Tuple[Model, dict]:\n",
    "    \"\"\"\n",
    "    Fit the model and return a tuple (fitted_model, history)\n",
    "    added in checkpoint as well\n",
    "    \"\"\"\n",
    "    print(Fore.BLUE + \"\\nTraining model...\" + Style.RESET_ALL)\n",
    "\n",
    "    es = callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=patience,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = '/home/honor/code/rs-uk/pictionary-ai/raw_data/models'\n",
    "    #this will save the checkpoints in the checkpoint_filepath\n",
    "    model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "    #in fit is where we put in the padding, cant remember how\n",
    "    history = model.fit(\n",
    "        X,\n",
    "        y,\n",
    "        validation_data=validation_data,\n",
    "        validation_split=validation_split,\n",
    "        epochs=50,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[es, model_checkpoint_callback],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Model trained on {len(X)} rows with min val accuracy: {round(np.min(history.history['accuracy']), 2)}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def evaluate_model(\n",
    "        model: Model,\n",
    "        X: np.ndarray,\n",
    "        y: np.ndarray,\n",
    "        batch_size=64\n",
    "    ) -> Tuple[Model, dict]:\n",
    "    \"\"\"\n",
    "    Evaluate trained model performance on the dataset\n",
    "    \"\"\"\n",
    "\n",
    "    print(Fore.BLUE + f\"\\nEvaluating model on {len(X)} rows...\" + Style.RESET_ALL)\n",
    "\n",
    "    if model is None:\n",
    "        print(f\"\\n❌ No model to evaluate\")\n",
    "        return None\n",
    "\n",
    "    metrics = model.evaluate(\n",
    "        x=X,\n",
    "        y=y,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        # callbacks=None,\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "    loss = metrics[\"loss\"]\n",
    "    mae = metrics[\"accuracy\"]\n",
    "\n",
    "    print(f\"✅ Model evaluated, accuracy: {round(mae, 2)}\")\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# use model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model initialized\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 85s 2s/step - loss: 0.1180 - accuracy: 0.9822 - val_loss: 1.3789e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 77s 2s/step - loss: 2.9557e-05 - accuracy: 1.0000 - val_loss: 9.7442e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 81s 3s/step - loss: 3.7823e-05 - accuracy: 1.0000 - val_loss: 9.4778e-05 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.6849e-05 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 1.\n",
      "32/32 [==============================] - 151s 5s/step - loss: 1.6849e-05 - accuracy: 1.0000 - val_loss: 9.3530e-05 - val_accuracy: 1.0000\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 8036 rows with min val accuracy: 0.98\n",
      "\u001b[34m\n",
      "Evaluating model on 4306 rows...\u001b[0m\n",
      "✅ Model evaluated, accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "#initialize model\n",
    "model = model_bidirectional()\n",
    "#compile model\n",
    "model= compile_model(model)\n",
    "#train model\n",
    "model, history = train_model(model, X_train, y_train, validation_data=[X_val,y_val])\n",
    "#evaluate model\n",
    "metrics=evaluate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 195\n",
      "✅ Model initialized\n",
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 24s 479ms/step - loss: 0.2597 - accuracy: 0.9780 - val_loss: 8.0146e-04 - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 14s 442ms/step - loss: 2.9099e-04 - accuracy: 1.0000 - val_loss: 2.4475e-04 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 15s 458ms/step - loss: 1.5092e-04 - accuracy: 1.0000 - val_loss: 2.2709e-04 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 1.4613e-04 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 1.\n",
      "32/32 [==============================] - 14s 430ms/step - loss: 1.4613e-04 - accuracy: 1.0000 - val_loss: 2.1984e-04 - val_accuracy: 1.0000\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 8036 rows with min val accuracy: 0.98\n",
      "\u001b[34m\n",
      "Evaluating model on 4306 rows...\u001b[0m\n",
      "✅ Model evaluated, accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "#initialize model\n",
    "model2 = model_LTSM()\n",
    "#compile model\n",
    "model2= compile_model(model2)\n",
    "#train model\n",
    "model2, history = train_model(model2, X_train, y_train, validation_data=[X_val,y_val])\n",
    "#evaluate model\n",
    "metrics=evaluate_model(model2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model compiled\n",
      "\u001b[34m\n",
      "Training model...\u001b[0m\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 17s 272ms/step - loss: 0.2188 - accuracy: 0.9716 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 7s 223ms/step - loss: 1.4322e-07 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 9s 271ms/step - loss: 6.8089e-08 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - ETA: 0s - loss: 5.0466e-08 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 1.\n",
      "32/32 [==============================] - 10s 302ms/step - loss: 5.0466e-08 - accuracy: 1.0000 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4: early stopping\n",
      "✅ Model trained on 8036 rows with min val accuracy: 0.97\n",
      "\u001b[34m\n",
      "Evaluating model on 4306 rows...\u001b[0m\n",
      "✅ Model evaluated, accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "#initialize model\n",
    "model3 = model_LTSM_conv()\n",
    "#compile model\n",
    "model3= compile_model(model3)\n",
    "#train model\n",
    "model3, history = train_model(model3, X_train, y_train, validation_data=[X_val,y_val])\n",
    "#evaluate model\n",
    "metrics=evaluate_model(model3, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observations\n",
    "- all have similiar accuracy, this si expected as only 2 classes\n",
    "- go in order for trianing times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ndjson\n",
      "  Downloading ndjson-0.3.1-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
      "Installing collected packages: ndjson\n",
      "Successfully installed ndjson-0.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install ndjson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_url = '../raw_data/trombone.ndjson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trombone\n"
     ]
    }
   ],
   "source": [
    "import ndjson\n",
    "\n",
    "with open(file_url,'r') as f:\n",
    "    for line in f:\n",
    "\n",
    "        ndjson.loads(line)\n",
    "\n",
    "        line_fixed = eval(line.replace('true','True'))\n",
    "\n",
    "        print(line_fixed['word'])\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bidirectional2() -> Model:\n",
    "    \"\"\"\n",
    "    Initialize the Neural Network with random weights, using bidirectional LTSM\n",
    "    masking layer\n",
    "    it has 2 Bidirectional LSTM layers\n",
    "    3 dense layers\n",
    "    and dropout layers\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Masking layer to handle variable-length sequences\n",
    "    #put in 99 as 0 may effect the data\n",
    "    model.add(layers.Masking(mask_value=99, input_shape=( max_length, 3)))\n",
    "\n",
    "    #do we want to customize backwards layer?\n",
    "\n",
    "    model.add(layers.Bidirectional(layers.LSTM(196, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)))\n",
    "    model.add(layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "    # Add Dense layers\n",
    "    model.add(layers.Dense(128, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(64, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    model.add(layers.Dense(32, activation='linear'))\n",
    "    #dropoutlayer\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    # Add final Softmax layer\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    # Replace 'num_classes' with the actual number of classes in your problem\n",
    "\n",
    "    print(\"✅ Model initialized\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-08 15:11:55.448804: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2024-03-08 15:11:55.448856: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2024-03-08 15:11:55.448889: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (Lenovo): /proc/driver/nvidia/version does not exist\n",
      "2024-03-08 15:11:55.449245: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model initialized\n",
      "✅ Model compiled\n"
     ]
    }
   ],
   "source": [
    "model1 =model_bidirectional2()\n",
    "\n",
    "model1 = compile_model(model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (None, 150, 3)            0         \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 150, 392)         313600    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 128)              233984    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                330       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 574,762\n",
      "Trainable params: 574,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.checkpoint.checkpoint.CheckpointLoadStatus at 0x7f28b0218700>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#called the folder and than what the model is called\n",
    "model1.load_weights('../raw_data/models/models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = list([[[0,22,37,64,255],[218,220,227,228,211]],[[76,95,135,141,150,159,166,180,186,201],[220,138,31,0,63,79,117,150,191,224]],[[94,104,111,119,127,141,143,142,180,191],[212,167,149,80,59,41,30,134,202,232]],[[109,127,137,147,150,162,172,185],[122,120,104,97,99,124,128,128]],[[75,130,158],[162,159,150]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_drawing_data(json_drawing: json) -> np.array:\n",
    "    '''\n",
    "    Extracts the drawing data (strokes list) from a drawing JSON file.\n",
    "    Transforms the strokes from coordinates to deltas.\n",
    "    Returns an np.array of deltas (d_x, d_y, end_of_stroke)\n",
    "    '''\n",
    "    # --- Data extraction ---\n",
    "    list_strokes = json_drawing\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    stroke_delimiter = []\n",
    "    list_points = [x, y, stroke_delimiter]\n",
    "\n",
    "    for stroke in list_strokes:\n",
    "        # Creating the third list to pass to the model with 0 all along a stroke and a 1 at the end of the stroke\n",
    "        stroke_delimiter = [0.] * len(stroke[0])\n",
    "        stroke_delimiter[-1] = 1\n",
    "        # Concatenating x, y, and the delimiter to the new list of points\n",
    "        list_points[0] += stroke[0]\n",
    "        list_points[1] += stroke[1]\n",
    "        list_points[2] += stroke_delimiter\n",
    "\n",
    "    np_points = np.asarray(list_points)\n",
    "    np_points = np_points.T\n",
    "\n",
    "    # --- Processing ---\n",
    "    # 1. Size normalization\n",
    "    lower = np.min(np_points[:, 0:2], axis=0) # returns (x_min, y_min)\n",
    "    upper = np.max(np_points[:, 0:2], axis=0) # returns (x_max, y_max)\n",
    "    scale = upper - lower # returns (width, heigth)\n",
    "    scale[scale == 0] = 1 # to escape a zero division for a vertical or horizontal stroke\n",
    "    np_points[:, 0:2] = (np_points[:, 0:2] - lower) / scale\n",
    "\n",
    "    # 2. Compute deltas\n",
    "    np_points[1:, 0:2] -= np_points[0:-1, 0:2]\n",
    "    np_points = np_points[1:, :]\n",
    "\n",
    "    return np.round(np_points,decimals=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eifell_tower = process_drawing_data(test)\n",
    "\n",
    "len(eifell_tower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padded_tensor(tensor) :\n",
    "    if len(tensor) >= 150 :\n",
    "        return tensor[0:150]\n",
    "    else :\n",
    "        pad_length = 150 - len(tensor)\n",
    "        padding = [[99,99,99]]\n",
    "        return np.concatenate((tensor,np.array(padding*pad_length)),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150, 3)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eifell = np.expand_dims(padded_tensor(eifell_tower),0)\n",
    "\n",
    "X_eifell.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 120ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model1.predict(X_eifell)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pictionary-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
